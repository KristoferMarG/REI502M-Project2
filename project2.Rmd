---
title: "Project 2"
author: 
  - Jón Þorsteinsson - jth56@hi.is
  - Kristófer Már Gíslason - kmg14@hi.is
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arules)
library(arulesViz)
library(funModeling)
library(RWeka)
```

### 1. Objectives

The data set we are working with predicts if a patient is likely or not to have a chronic kidney disease based on various attributes that are all based on different medical tests. The objective of the project is to use association rule mining to identify patterns between different attributes of the data set. If the patterns we find are good enough it might be possible for doctors to identify certain patterns between symptoms and maybe skip certain tests on patients which could then hopefully help them when deciding on which tests could be more usefull than others when diagnosing a patient. The rule set could also be used to fill in some missing data in the data set which could then be used to get a better classification for the data.

### 2. Data set description

The data set has 400 different instances that each contain 24 different attributes and one class attribute that tells if the patient has the disease or not. The attributes are:

| Nr. | Name | Type | Description |
| --- | ---- | ---- | ----------- |
| 1. | Age | numerical | Age in years. |
| 2. | Blood Pressure | numerical | bp in mm/Hg. |
| 3. | Specific Gravity | nominal | sg - (1.005,1.010,1.015,1.020,1.025). |
| 4. | Albumin | nominal | al - (0,1,2,3,4,5). |
| 5. | Sugar | nominal | su - (0,1,2,3,4,5). |
| 6. | Red Blood Cells | nominal | rbc - (normal,abnormal)|
| 7. | Pus Cell | nominal | pc - (normal,abnormal) |
| 8. | Pus Cell clumps | nominal | pcc - (present,notpresent)|
| 9. | Bacteria | nominal | ba - (present,notpresent)|
| 10. | Blood Glucose Random | numerical | bgr in mgs/dl| 
| 11. | Blood Urea | numerical | bu in mgs/dl| 
| 12. | Serum Creatinine | numerical | sc in mgs/dl| 
| 13. | Sodium | numerical | sod in mEq/L| 
| 14. | Potassium | numerical | pot in mEq/L| 
| 15. | Hemoglobin | numerical | hemo in gms| 
| 16. | Packed Cell Volume | numerical | pcv| 
| 17. | White Blood Cell Count | numerical | wc in cells/cumm| 
| 18. | Red Blood Cell Count | numerical | rc in millions/cmm| 
| 19. | Hypertension | nominal | htn - (yes,no)| 
| 20. | Diabetes Mellitus | nominal | dm - (yes,no)| 
| 21. | Coronary Artery Disease | nominal | cad - (yes,no)| 
| 22. | Appetite | nominal | appet - (good,poor)| 
| 23. | Pedal Edema | nominal | pe - (yes,no)|	
| 24. | Anemia | nominal | ane - (yes,no)| 
| 25. | Class | nominal | class - (ckd,notckd)|


#Preprocessing

We decided to try 2 different ways of discretizising our numerical variables using equal frequency and equal intervals. In our opinion it was not necessary to remove / ignore any attributes because each of the attributes was a specific medical test and therefore might have some valuable information. After running the first tests we saw that the class atttribute was negative on the right hand side of all of our highest ranked rules and since those results were not very interesting to us in this assignment we decided to remove it.  We tried a few different values for the number of bins in the discretization but it didn't seem to change that much for our data so we decided on having 10 bins.


```{r data, tidy = TRUE}
# read in .arff file
chronic <- read.arff("chronic_kidney_disease_full.arff")

# dicretize using equal frequency
chronic_freq <- discretizeDF(chronic, methods = list(
  age = list(method = "frequency", breaks = 10 ),
  bp  = list(method = "frequency", breaks = 3 ),
  bgr = list(method = "frequency", breaks = 10 ),
  bu  = list(method = "frequency", breaks = 10 ),
  sc  = list(method = "frequency", breaks = 10 ),
  sod = list(method = "frequency", breaks = 10 ),
  pot = list(method = "frequency", breaks = 10 ),
  hemo= list(method = "frequency", breaks = 10 ),
  pcv = list(method = "frequency", breaks = 10 ),
  wbcc= list(method = "frequency", breaks = 10 ),
  rbcc= list(method = "frequency", breaks = 10 )
  ))

# dicretize using equal interval
chronic_interval <- discretizeDF(chronic, methods = list(
  age = list(method = "interval", breaks = 10 ),
  bp  = list(method = "interval", breaks = 10 ),
  bgr = list(method = "interval", breaks = 10 ),
  bu  = list(method = "interval", breaks = 10 ),
  sc  = list(method = "interval", breaks = 10 ),
  sod = list(method = "interval", breaks = 10 ),
  pot = list(method = "interval", breaks = 10 ),
  hemo= list(method = "interval", breaks = 10 ),
  pcv = list(method = "interval", breaks = 10 ),
  wbcc= list(method = "interval", breaks = 10 ),
  rbcc= list(method = "interval", breaks = 10 )
  ))

chronic_freq$class <- NULL
chronic_interval$class <- NULL
```

### 3. Rule mining process

Since the data set is not very big we decided to put the minimum support threshold for the Apriori algorithm at 0.1 to prevent getting results that could very well be coincidental. We also decided to set the confidence factor pretty high at 0.9 since we are only interested in very strong rules. Since we have so many attributes with many different values we decided to limit the length of the rules to 4 to avoid getting rules that would be to complicated. We ran the apiori algorithm for the 3 differently discretizised data sets and compared the results.

```{r seconddata, results="hide",warning=FALSE}

freq.rules <- apriori(chronic_freq, parameter=list(support = 0.1, 
              confidence = 0.9, minlen=2, maxlen=4))

interval.rules <- apriori(chronic_interval, parameter=list(support = 0.1,
                  confidence = 0.9, minlen=2, maxlen=4))

```

```{r summary1}

sum.freq <- summary(freq.rules)
sum.interval <- summary(interval.rules)

sum.freq@quality
sum.freq@lengths
sum.interval@quality
sum.interval@lengths

```

The mean for confidence is pretty similar in both cases the interval discretization has a little bit higher average lift. The biggest difference is between support but that is probably because the rule set which we get with the interval discretization is much bigger than the other one and therefor probably has many low supported rules.


```{r, warning = FALSE}
plot(freq.rules)
plot(interval.rules)
```


```{r summary, warning=FALSE}
inspect(head(freq.rules, n = 30, by ="lift"))

inspect(head(interval.rules, n = 30, by ="lift"))


```
```{r}
interval.rules <- apriori(chronic_interval, parameter=list(support = 0.1,
                  confidence = 0.95, minlen=2, maxlen=2))

summary(interval.rules)

inspect(head(interval.rules, n = 30, by ="lift"))

```


### 4. Results + Recommendations

Resulting rules: Summary (number of rules, general description), and a selection of those
you would show to a client.

Recommendations: What should the client do because of the rules discovered.

[4]   {al=0,rbc=normal}              => {pc=normal}      0.3525  1.0000000  1.544402 141

In our calculations we got rules with very high confidence and pretty low support this means that the rule occurs infrequently while indicating that the rule has a high predictive power, which in our case is important. ...



