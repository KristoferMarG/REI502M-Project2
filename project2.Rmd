---
title: "Project 2 - Association Rules"
author: 
  - Jón Þorsteinsson - jth56@hi.is
  - Kristófer Már Gíslason - kmg14@hi.is
output: 
  html_document:
    css: styles.css
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(arules)
library(arulesViz)
library(funModeling)
library(RWeka)

```

### 1. Objectives
The data set we are working with predicts if a patient is likely or not to have a chronic kidney disease based on various attributes that are all based on different medical tests. The objective of the project is to use association rule mining to identify patterns between different attributes of the data set. If the patterns we find are good enough it might be possible for doctors to identify certain patterns between symptoms and maybe skip certain tests on patients which could then hopefully help them when deciding on which tests could be more usefull than others when diagnosing a patient. 
The rule set could also be used to fill in some missing data in the data set which could then be used to get a better classification for the data.

### 2. Data set description

The data set has 400 different instances that each contain 24 different attributes and one class attribute that tells if the patient has the disease or not. The attributes are:

| Nr. | Name | Type | Description |
| --- | ---- | ---- | ----------- |
| 1. | Age | numerical | Age in years. |
| 2. | Blood Pressure | numerical | bp in mm/Hg. |
| 3. | Specific Gravity | nominal | sg - (1.005,1.010,1.015,1.020,1.025). |
| 4. | Albumin | nominal | al - (0,1,2,3,4,5). |
| 5. | Sugar | nominal | su - (0,1,2,3,4,5). |
| 6. | Red Blood Cells | nominal | rbc - (normal,abnormal)|
| 7. | Pus Cell | nominal | pc - (normal,abnormal) |
| 8. | Pus Cell clumps | nominal | pcc - (present,notpresent)|
| 9. | Bacteria | nominal | ba - (present,notpresent)|
| 10. | Blood Glucose Random | numerical | bgr in mgs/dl| 
| 11. | Blood Urea | numerical | bu in mgs/dl| 
| 12. | Serum Creatinine | numerical | sc in mgs/dl| 
| 13. | Sodium | numerical | sod in mEq/L| 
| 14. | Potassium | numerical | pot in mEq/L| 
| 15. | Hemoglobin | numerical | hemo in gms| 
| 16. | Packed Cell Volume | numerical | pcv| 
| 17. | White Blood Cell Count | numerical | wc in cells/cumm| 
| 18. | Red Blood Cell Count | numerical | rc in millions/cmm| 
| 19. | Hypertension | nominal | htn - (yes,no)| 
| 20. | Diabetes Mellitus | nominal | dm - (yes,no)| 
| 21. | Coronary Artery Disease | nominal | cad - (yes,no)| 
| 22. | Appetite | nominal | appet - (good,poor)| 
| 23. | Pedal Edema | nominal | pe - (yes,no)|	
| 24. | Anemia | nominal | ane - (yes,no)| 
| 25. | Class | nominal | class - (ckd,notckd)|


#Preprocessing

We decided to try 2 different ways of discretizising our numerical variables using equal frequency and equal intervals. In our opinion it was not necessary to remove / ignore any attributes because each of the attributes was a specific medical test and therefore might have some valuable information. After running the first tests we saw that the class atttribute was negative on the right hand side of all of our highest ranked rules and since those results were not very interesting to us in this assignment we decided to remove it.  We tried a few different values for the number of bins in the discretization but it didn't seem to change that much for our data so we decided on having 10 bins.


```{r data, tidy = TRUE}
# read in .arff file
chronic <- read.arff("chronic_kidney_disease_full.arff")

# discretize using equal frequency
chronic_freq <- discretizeDF(chronic, methods = list(
  age = list(method = "frequency", breaks = 10 ),
  bp  = list(method = "frequency", breaks = 3 ),
  bgr = list(method = "frequency", breaks = 10 ),
  bu  = list(method = "frequency", breaks = 10 ),
  sc  = list(method = "frequency", breaks = 10 ),
  sod = list(method = "frequency", breaks = 10 ),
  pot = list(method = "frequency", breaks = 10 ),
  hemo= list(method = "frequency", breaks = 10 ),
  pcv = list(method = "frequency", breaks = 10 ),
  wbcc= list(method = "frequency", breaks = 10 ),
  rbcc= list(method = "frequency", breaks = 10 )
  ))

# discretize using equal interval
chronic_interval <- discretizeDF(chronic, methods = list(
  age = list(method = "interval", breaks = 10 ),
  bp  = list(method = "interval", breaks = 10 ),
  bgr = list(method = "interval", breaks = 10 ),
  bu  = list(method = "interval", breaks = 10 ),
  sc  = list(method = "interval", breaks = 10 ),
  sod = list(method = "interval", breaks = 10 ),
  pot = list(method = "interval", breaks = 10 ),
  hemo= list(method = "interval", breaks = 10 ),
  pcv = list(method = "interval", breaks = 10 ),
  wbcc= list(method = "interval", breaks = 10 ),
  rbcc= list(method = "interval", breaks = 10 )
  ))

# Removing the class attribute.
chronic_freq$class <- NULL
chronic_interval$class <- NULL
```

### 3. Rule mining process

Since the data set is not very big we decided to put the minimum support threshold for the Apriori algorithm at 0.1 to prevent getting results that could very well be coincidental. We also decided to set the confidence factor pretty high at 0.9 since we are only interested in very strong rules. Since we have so many attributes with many different values we decided to limit the length of the rules to 4 to avoid getting rules that would be to complicated. We ran the apiori algorithm for the 3 differently discretizised data sets and compared the results.

```{r seconddata, results="hide",warning=FALSE}
# Running the apriori algorithm.
freq.rules <- apriori(chronic_freq, parameter=list(support = 0.1, 
              confidence = 0.9, minlen=2, maxlen=4))

interval.rules <- apriori(chronic_interval, parameter=list(support = 0.1,
                  confidence = 0.9, minlen=2, maxlen=4))
```

```{r summary1}
sum.freq <- summary(freq.rules)
sum.interval <- summary(interval.rules)

# Information about the equal frequency rule set
sum.freq@quality
# Size of the equal frequancy rule set
sum.freq@lengths
# Information about the equal interval rule set
sum.interval@quality
# Size of the equal interval rule set
sum.interval@lengths

# Scatter plot for the equal frequency rule set
plot(freq.rules)
# Scatter plot for the equal interval rule set
plot(interval.rules)
```

The mean for confidence is pretty similar in both cases the interval discretization has a little bit higher average lift. The biggest difference is between support but that is probably because the rule set which we get with the interval discretization is much bigger than the other one and therefore probably has many low supported rules. Ploting the results we could see that the equal frequency discretization gave us fewer rules and fewer rules with high lift however the equal interval discretization gave us way too many rules but more rules with higher lift.

Looking at our data we saw that some of our attributes had a wide range of values but the most common values were all at the same small interval. For that reason we decided to use a change our discretization and use equal frequency for those attributes and equal intervals for the other ones. With that approach we got higher average confidence and lift and a more manageble rule set than by using only equal intervals. Looking at the plot we can also see that we get rid of alot of uninteresting rules from our equal interval rule set but get more rules with high lift than in our equal frequency rule set. It is also interesting that we have don't seem to have any rules between about 0.25 to 0.32 in support and only a few over 0.32 support with high lift.

```{r results="hide",warning=FALSE}
# Discretizising with a mixed method
chronicDisc <- discretizeDF(chronic, methods = list(
  age = list(method = "interval", breaks = 10 ),
  bp  = list(method = "interval", breaks = 10 ),
  bgr = list(method = "frequency", breaks = 10 ),
  bu  = list(method = "frequency", breaks = 10 ),
  sc  = list(method = "frequency", breaks = 10 ),
  sod = list(method = "frequency", breaks = 10 ),
  pot = list(method = "frequency", breaks = 10 ),
  hemo= list(method = "interval", breaks = 10 ),
  pcv = list(method = "interval", breaks = 10 ),
  wbcc= list(method = "frequency", breaks = 10 ),
  rbcc= list(method = "interval", breaks = 10 )
  ))

chronicDisc$class <- NULL

# Running the apriori algorithm on our new data set
chronic.rules <- apriori(chronicDisc, parameter=list(support = 0.1, 
              confidence = 0.9, minlen=2, maxlen=4))
```

```{r, warning = FALSE}
sum.chronic <- summary(chronic.rules)
# Statistics for the new rule set
sum.chronic@quality
# Size of the new rule set
sum.chronic@lengths
```
```{r}
# Scatter plot for our new rule set.
plot(chronic.rules)
```


```{r summary, warning=FALSE}
# Getting the 20 rules with the highest lift
inspect(head(chronic.rules, n = 20, by ="lift"))
```

Looking at the highest ranked rules by lift we can see that they all share 2-3 items and they all have *{al=0}* on the right hand side and most of them have *{rbc=normal}* and *{pcv=[45,49.5]}* on the left hand side. This is not very usefull to us but it is likely that we could get equally as good shorter rules by decreasing the maximum length of our rules. By looking at a two-key plot for our rules we can also see that the rules of length 4 seem to be very similar to the rules of length 3.

```{r warning=FALSE}

plot(chronic.rules, method = "two-key plot")
```



```{r results="hide",warning=FALSE}
# Running the apriori algorithm again with maxlen=3
chronic.rules <- apriori(chronicDisc, parameter=list(support = 0.1, 
              confidence = 0.9, minlen=2, maxlen=3))
```


```{r}
# Gettting the best lift rules with maxlen=3
inspect(head(chronic.rules, n = 20, by ="lift"))
```

The resulting rules are shorter which is better but they still contain mostly the same items just like the first rule set. Since they all have low support and most of our high lift rules have low support we assume that these attributes have something to do with similar symptoms that are also not that common and might not be that interesting. We decided to increase the support threshold of our algorithm to 0.3 because we suspect that the rules that are over that threshold could be more interesting and give us a more focused view of the data.

```{r results="hide",warning=FALSE}
# Running the apriori algorithm again with maxlen=3 and support=0.3
chronic.rules <- apriori(chronicDisc, parameter=list(support = 0.3, 
              confidence = 0.9, minlen=2, maxlen=3))
```
```{r}
# Gettting the best lift rules with maxlen=3 and support=0.3
inspect(head(chronic.rules, n = 20, by ="lift"))
```

Now we get rules with a little bit lower lift and confidence but both factors are still pretty high and the support is much higher. Most of the rules contain the same parameters which are also the same ones as we got in most of the rules when running the algorithm with a lower support threshold. The same is true if we look at more rules so we decided not to show more than 20 of them.


```{r}
sum.chronic <- summary(chronic.rules)
# Statistics for the new rule set
sum.chronic@quality
# Size of the new rule set
sum.chronic@lengths
```



### 4. Results and Recommendations

The final rule set we got has 373 rules. The average support is **0.52**, average confidence **0.95** and an average lift of **1.13**. The highest ranking rules we got are all related to albumin levels, pus cell status, red blood cell status and hypertension which suggests that there could be a strong connection between these factors. With our lack of knowledge in medical science it is hard for us to say if these results would be interesting or not and if some rules are more interesting than others without basing our opinion only on the numbers. For that reason we do not want to make any assumption on whether if any rules are more interesting than others and would recommend that someone more knowledgeable about the subject would make that decision.





